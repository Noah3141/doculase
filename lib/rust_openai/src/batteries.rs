use std::str::FromStr;
use serde::{Serialize,Deserialize};


/// The options for which set of questions to propose to the PDF summary endpoints. Each variant corresponds to a JSON blob which is used as prompt
/// Each of the variants has a corresponding struct which is used to intake the GPT response content (the structure of these structs can be autogenerated from the json strings defined in Battery's `.to_prompt( )`  )
#[derive(Clone, Copy)]
pub enum Battery {
    Essay, 
    CompleteVoynich,
    //VonyichHardData, // Explicitly Encoded Data: Could be designed to ask for the "in the text" data, stopping at from-text-quotes for complex questions, and mainly ask for explicit data "title", "journal", "authors"
    //VoynichSoftData // Implicity Encoded Data: Could be designed to ask for more fluid generations, at higher temperature, such as "What are the authors forgetting"
    
    MetConsensus, // Met- prefix intended to be understood as "battery which runs on output of other batteries"
}

impl FromStr for Battery {
    type Err = String;
    /// Generate the corresponding enum variant from a `&str`
    /// ```
    /// impl FromStr for Battery {
    ///     type Err = String;
    ///     fn from_str(s: &str) -> Result<Self, Self::Err> {
    ///         match s {
    ///         "basic" => Ok(Battery::Basic),
    /// ...
    /// }
    /// ```
    /// Generally, the format of accepted `&str`'s is meant to accomodate their origin from a dynamic URL slug â€” kebab case
    fn from_str(s: &str) -> Result<Self, Self::Err> {
        match s {
            "essay" => Ok(Battery::Essay),
            "complete-voynich" => Ok(Battery::CompleteVoynich),
            "met-consensus" => Ok(Battery::MetConsensus),
            _ => return Err("Either `from_str() for Battery` needs to be synced to Battery's variants, or you've provided an invalid battery type. You should POST to localhost:port/pdf-summary/run-battery/<battery>, where battery is one of the strings implemented in `FromStr` for `Battery`".to_string())
        }
    }

}

impl Battery {
    /// Convert this battery variant into a corresponding GPT prompt, that is used to ask a number of questions simultaneously about the PDF.
    pub fn to_prompt(&self, doc: String) -> String {

        match self {
            Battery::Essay => {
                let battery_text = std::fs::read_to_string("./openai_for_rs/src/models/batteries/Essay.txt").expect("reading battery from its text file");
                format!("{battery_text} \n\n {doc}")
            },
            Battery::CompleteVoynich => {
                let battery_text = std::fs::read_to_string("./openai_for_rs/src/models/batteries/CompleteVoynich.txt").expect("reading battery from its text file");
                format!("{battery_text} \n\n {doc}")
            },
            Battery::MetConsensus => {
                let battery_text = std::fs::read_to_string("./openai_for_rs/src/models/batteries/MetConsensus.txt").expect("reading battery from its text file");
                format!("{battery_text} \n\n {input}", input = doc)
            }
        }.to_string()

    }

    /// Convert this battery variant into a corresponding string label, which is paired with file title to make a cache key for PdfSummaries. This allows us to not put entire documents or prompts inside the cache, but rather tether them by filename, keeping the cache readable, and allowing indefinitely large prompts,
    /// <br><br>
    /// ```
    /// OpenAIAccount { cache: HashMap<String, Query>, ...} 
    /// Query { prompt: Battery::Minimal.as_prompt_stampt(), ...}
    /// 
    /// Battery::Minimal => { "Minimal Summary Battery" }
    /// Battery::Basic => { "Basic Summary Battery" }, 
    /// Battery::Comprehensive => { "Comprehensive Summary Battery" }
    /// ```
    /// <br> Note that for chat completion Queries, the cache key and the `prompt` attribute of the Query at that key MATCH, whereas a Pdf Summary looks like:
    /// ```
    /// {
    ///    "Cinnamon - Minimal Summary Battery": {
    ///        "prompt": "Minimal Summary Battery",
    ///        "response": {
    ///        "id": "chatcmpl-7YkXphbSUwgVRcpRaoHA3tipMrie7",
    ///        "object": "chat.completion",
    ///        "created": 1688516425,
    ///        "model": "gpt-3.5-turbo-16k-0613",
    ///        "choices": [ ...
    /// ```
    /// For example, compare a PromptCompletion query with a PdfCompletion query:
    /// ```
    /// client.cache["whats the deal with airline food"] 
    /// == Query { 
    ///     prompt: "whats the deal with airline food", 
    ///     query_type: QueryType::PromptCompletion,
    ///     ...
    /// }
    /// ```
    /// whereas
    /// ```
    /// client.cache["Cinnamon and its Effects on Diabetes in Mice with Low Self Esteem and Super Alzheimer's - Minimal Summary Battery"] 
    /// == Query { 
    ///     prompt: "Minimal Summary Battery", 
    ///     query_type: QueryType:PdfCompletion,
    ///     ...
    /// }
    /// ```
    pub fn as_prompt_stamp(&self) -> String {
        match self { 
    Battery::Essay => { "Essay Battery" }, 
    Battery::CompleteVoynich => { "Complete Voynich Battery" },  
    Battery::MetConsensus => { "Consensus Meta-Battery" }, // The stamp must contain "Meta-Battery" to be read from database properly
    }.to_string()
    }
    
    
    // pub fn deserialize_response(&self, json: String) 
    // -> BatteryResponse<
    // MinimalResponse, BasicResponse, ComprehensiveResponse, VoynichResponse, PsychReviewQualityResponse
    // > {
    //     use BatteryResponse::*;
    //     use serde_json::from_str;
    //     let json = json.as_str();
    //     match self {
    //         Battery::CompleteVoynich => CompleteVoynich(from_str::<VoynichResponse>(json).unwrap()),
    //     }
    // }


}

pub enum BatteryResponse<
    A, B, C, D, E
> {
    Minimal(A),
    Basic(B),
    Comprehensive(C),
    CompleteVoynich(D),
    PsychReviewQuality(E),
}


#[derive(Serialize,Deserialize)]
pub struct MinimalResponse {
    r#abstract: String,
    publication_date: String,
}


#[derive(Serialize,Deserialize)]
pub struct BasicResponse {
    title: String,
    authors: Vec<String>,
    methods: String,
    results: String,
    conclusions: String,
    further_research: String,
    keywords: Vec<String>,
}

#[derive(Serialize,Deserialize)]
pub struct ComprehensiveResponse {}

#[derive(Serialize,Deserialize)]
pub struct VoynichResponse {}

#[derive(Serialize,Deserialize)]
pub struct PsychReviewQualityResponse {
    hypothesis_presence: f32,
    mechanism_presence: f32,
    balance: f32
}